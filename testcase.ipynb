{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyPkRHcbkJJ5FE/nEGS8P8r0",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/aditya2k5/Forecasting-electric-load-using_ML/blob/main/testcase.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 387
        },
        "id": "ubUu8xjjUXQz",
        "outputId": "5d7d6fce-44e0-4d10-9bee-7d6e1814b63f"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "[Errno 2] No such file or directory: 'loadpredictiondataset.csv'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-1-2939636485.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mseaborn\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0msns\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'loadpredictiondataset.csv'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     20\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'Datetime'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_datetime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'Datetime'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mformat\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'%m/%d/%Y %H:%M'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'Hour'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'Datetime'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhour\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[0m\n\u001b[1;32m   1024\u001b[0m     \u001b[0mkwds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkwds_defaults\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1025\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1026\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1027\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1028\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    618\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    619\u001b[0m     \u001b[0;31m# Create the parser.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 620\u001b[0;31m     \u001b[0mparser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    621\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    622\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m   1618\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1619\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhandles\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mIOHandles\u001b[0m \u001b[0;34m|\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1620\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1621\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1622\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[0;34m(self, f, engine)\u001b[0m\n\u001b[1;32m   1878\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0;34m\"b\"\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1879\u001b[0m                     \u001b[0mmode\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;34m\"b\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1880\u001b[0;31m             self.handles = get_handle(\n\u001b[0m\u001b[1;32m   1881\u001b[0m                 \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1882\u001b[0m                 \u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/io/common.py\u001b[0m in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    871\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencoding\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;34m\"b\"\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    872\u001b[0m             \u001b[0;31m# Encoding\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 873\u001b[0;31m             handle = open(\n\u001b[0m\u001b[1;32m    874\u001b[0m                 \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    875\u001b[0m                 \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'loadpredictiondataset.csv'"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.tree import DecisionTreeRegressor\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "from xgboost import XGBRegressor\n",
        "from sklearn.svm import SVR\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import LSTM, Dense\n",
        "from tensorflow.keras.callbacks import EarlyStopping\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import mean_squared_error, r2_score\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.linear_model import SGDRegressor\n",
        "import json\n",
        "import seaborn as sns\n",
        "\n",
        "df = pd.read_csv('loadpredictiondataset.csv')\n",
        "df['Datetime'] = pd.to_datetime(df['Datetime'], format='%m/%d/%Y %H:%M')\n",
        "df['Hour'] = df['Datetime'].dt.hour\n",
        "df['Day'] = df['Datetime'].dt.day\n",
        "df['Month'] = df['Datetime'].dt.month\n",
        "df = df.dropna()\n",
        "#standardizing\n",
        "scaler = StandardScaler()\n",
        "X_scaled = scaler.fit_transform(X)\n",
        "#spliting dataste\n",
        "X_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Data inspection\n",
        "print(\"Data Summary:\")\n",
        "print(df.describe())\n",
        "print(\"\\nMissing Values:\")\n",
        "print(df.isnull().sum())\n",
        "\n",
        "# Correlation matrix\n",
        "plt.figure(figsize=(10, 8))\n",
        "sns.heatmap(df[features + ['PowerConsumption_Zone1']].corr(), annot=True, cmap='coolwarm', vmin=-1, vmax=1)\n",
        "plt.title('Feature Correlation Matrix')\n",
        "plt.show()\n",
        "\n",
        "# Outlier removal using IQR\n",
        "def remove_outliers(df, column):\n",
        "    Q1 = df[column].quantile(0.25)\n",
        "    Q3 = df[column].quantile(0.75)\n",
        "    IQR = Q3 - Q1\n",
        "    lower_bound = Q1 - 1.5 * IQR\n",
        "    upper_bound = Q3 + 1.5 * IQR\n",
        "    return df[(df[column] >= lower_bound) & (df[column] <= upper_bound)]\n",
        "\n",
        "# Apply outlier removal to target variable\n",
        "df_clean = remove_outliers(df, 'PowerConsumption_Zone1')\n",
        "print(f\"Rows after outlier removal: {len(df_clean)} (original: {len(df)})\")\n",
        "\n",
        "# Linear regression\n",
        "features = ['Temperature', 'Humidity', 'WindSpeed', 'GeneralDiffuseFlows', 'DiffuseFlows', 'Hour', 'Day', 'Month','PowerConsumption_Zone1']\n",
        "X = df_clean[features]\n",
        "y = df_clean['PowerConsumption_Zone1']\n",
        "\n",
        "# Standardize features\n",
        "scaler = StandardScaler()\n",
        "X_scaled = scaler.fit_transform(X)\n",
        "\n",
        "# Standardize target variable\n",
        "y_scaler = StandardScaler()\n",
        "y_scaled = y_scaler.fit_transform(y.values.reshape(-1, 1)).flatten()\n",
        "\n",
        "# Split data\n",
        "X_train, X_test, y_train, y_test = train_test_split(X_scaled, y_scaled, test_size=0.2, random_state=42)\n",
        "\n",
        "# Custom Linear Regression with Gradient Descent and L2 Regularization\n",
        "class CustomLinearRegression:\n",
        "    def __init__(self, learning_rate=0.0008, epochs=1500, l2_lambda=0.05):\n",
        "        self.learning_rate = learning_rate\n",
        "        self.epochs = epochs\n",
        "        self.l2_lambda = l2_lambda\n",
        "        self.weights = None\n",
        "        self.bias = None\n",
        "        self.losses = []\n",
        "\n",
        "    def fit(self, X, y):\n",
        "        n_samples, n_features = X.shape\n",
        "        self.weights = np.zeros(n_features)\n",
        "        self.bias = 0\n",
        "\n",
        "        for epoch in range(self.epochs):\n",
        "            y_pred = np.dot(X, self.weights) + self.bias\n",
        "            mse_loss = np.mean((y_pred - y) ** 2)\n",
        "            l2_loss = self.l2_lambda * np.sum(self.weights ** 2)\n",
        "            loss = mse_loss + l2_loss\n",
        "            self.losses.append(loss)\n",
        "\n",
        "            dw = (1/n_samples) * np.dot(X.T, (y_pred - y)) + 2 * self.l2_lambda * self.weights\n",
        "            db = (1/n_samples) * np.sum(y_pred - y)\n",
        "\n",
        "            self.weights -= self.learning_rate * dw\n",
        "            self.bias -= self.learning_rate * db\n",
        "\n",
        "            if (epoch + 1) % 100 == 0:\n",
        "                print(f'Epoch {epoch + 1}/{self.epochs}, Loss: {loss:.4f}')\n",
        "\n",
        "    def predict(self, X):\n",
        "        return np.dot(X, self.weights) + self.bias\n",
        "\n",
        "# Train model\n",
        "model = CustomLinearRegression(learning_rate=0.0008, epochs=1500, l2_lambda=0.05)\n",
        "model.fit(X_train, y_train)\n",
        "y_pred_scaled = model.predict(X_test)\n",
        "\n",
        "# Inverse transform for evaluation\n",
        "y_pred = y_scaler.inverse_transform(y_pred_scaled.reshape(-1, 1)).flatten()\n",
        "y_test_orig = y_scaler.inverse_transform(y_test.reshape(-1, 1)).flatten()\n",
        "\n",
        "# Testing\n",
        "mse = mean_squared_error(y_test_orig, y_pred)\n",
        "r2 = r2_score(y_test_orig, y_pred)\n",
        "print(f'Mean Squared Error: {mse:.2f}')\n",
        "print(f'R² Score: {r2:.2f}')\n",
        "\n",
        "# Plotting actual vs predicted\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.plot(y_test_orig[:2000], label='Actual Power Consumption', marker='o')\n",
        "plt.plot(y_pred[:2000], label='Predicted Power Consumption', marker='x')\n",
        "plt.title('Actual vs Predicted Power Consumption (Zone 1)')\n",
        "plt.xlabel('Sample Index')\n",
        "plt.ylabel('Power Consumption')\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "plt.show()\n",
        "\n",
        "# Plotting loss over epochs\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.plot(range(1, len(model.losses) + 1), model.losses)\n",
        "plt.title('Training Loss vs Epochs')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Mean Squared Error Loss (Scaled)')\n",
        "plt.grid(True)\n",
        "plt.show()\n",
        "\n",
        "# Data inspection\n",
        "print(\"Data Summary:\")\n",
        "print(df.describe())\n",
        "print(\"\\nMissing Values:\")\n",
        "print(df.isnull().sum())\n",
        "\n",
        "# Features (consistent with previous code)\n",
        "features = ['Temperature', 'Humidity', 'WindSpeed', 'GeneralDiffuseFlows', 'DiffuseFlows', 'Hour', 'Day', 'Month','PowerConsumption_Zone1']\n",
        "X = df[features]\n",
        "y = df['PowerConsumption_Zone1']\n",
        "\n",
        "# Standardize features\n",
        "scaler = StandardScaler()\n",
        "X_scaled = scaler.fit_transform(X)\n",
        "\n",
        "# Standardize target variable\n",
        "y_scaler = StandardScaler()\n",
        "y_scaled = y_scaler.fit_transform(y.values.reshape(-1, 1)).flatten()\n",
        "\n",
        "# Split data\n",
        "X_train, X_test, y_train, y_test = train_test_split(X_scaled, y_scaled, test_size=0.2, random_state=42)\n",
        "\n",
        "# Simulate epochs by incrementally increasing training data\n",
        "n_epochs = 100\n",
        "train_losses = []\n",
        "val_losses = []\n",
        "n_samples = len(X_train)\n",
        "step_size = max(1, n_samples // n_epochs)  # Ensure at least 1 sample per step\n",
        "\n",
        "# Initialize model\n",
        "model = DecisionTreeRegressor(max_depth=10, random_state=42)\n",
        "\n",
        "for epoch in range(n_epochs):\n",
        "    # Use increasing subsets of training data\n",
        "    end_idx = min((epoch + 1) * step_size, n_samples)\n",
        "    X_train_subset = X_train[:end_idx]\n",
        "    y_train_subset = y_train[:end_idx]\n",
        "\n",
        "    # Train model on subset\n",
        "    model.fit(X_train_subset, y_train_subset)\n",
        "\n",
        "    # Compute training and validation loss\n",
        "    y_pred_train = model.predict(X_train_subset)\n",
        "    y_pred_val = model.predict(X_test)\n",
        "    train_loss = mean_squared_error(y_train_subset, y_pred_train)\n",
        "    val_loss = mean_squared_error(y_test, y_pred_val)\n",
        "    train_losses.append(train_loss)\n",
        "    val_losses.append(val_loss)\n",
        "\n",
        "    # Print progress every 10 epochs\n",
        "    if (epoch + 1) % 10 == 0:\n",
        "        print(f'Epoch {epoch + 1}/{n_epochs}, Train Loss: {train_loss:.4f}, Val Loss: {val_loss:.4f}')\n",
        "\n",
        "# Final model training on full training data\n",
        "model.fit(X_train, y_train)\n",
        "y_pred_scaled = model.predict(X_test)\n",
        "\n",
        "# Inverse transform for evaluation\n",
        "y_pred = y_scaler.inverse_transform(y_pred_scaled.reshape(-1, 1)).flatten()\n",
        "y_test_orig = y_scaler.inverse_transform(y_test.reshape(-1, 1)).flatten()\n",
        "\n",
        "# Testing\n",
        "mse = mean_squared_error(y_test_orig, y_pred)\n",
        "r2 = r2_score(y_test_orig, y_pred)\n",
        "print(f'Mean Squared Error: {mse:.2f}')\n",
        "print(f'R² Score: {r2:.2f}')\n",
        "\n",
        "# Plotting actual vs predicted\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.plot(y_test_orig[:2000], label='Actual Power Consumption', marker='o')\n",
        "plt.plot(y_pred[:2000], label='Predicted Power Consumption', marker='x')\n",
        "plt.title('Actual vs Predicted Power Consumption (Zone 1) - Decision Tree')\n",
        "plt.xlabel('Sample Index')\n",
        "plt.ylabel('Power Consumption')\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "plt.show()\n",
        "\n",
        "# Plotting loss over epochs\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.plot(range(1, n_epochs + 1), train_losses, label='Training Loss')\n",
        "plt.plot(range(1, n_epochs + 1), val_losses, label='Validation Loss')\n",
        "plt.title('Training and Validation Loss vs Epochs')\n",
        "plt.xlabel('Epoch (Data Subset Iteration)')\n",
        "plt.ylabel('Mean Squared Error Loss (Scaled)')\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "plt.show()\n",
        "\n",
        "# Data inspection\n",
        "print(\"Data Summary:\")\n",
        "print(df.describe())\n",
        "print(\"\\nMissing Values:\")\n",
        "print(df.isnull().sum())\n",
        "\n",
        "# Features (consistent with previous code)\n",
        "features = ['Temperature', 'Humidity', 'WindSpeed', 'GeneralDiffuseFlows', 'DiffuseFlows', 'Hour', 'Day', 'Month','PowerConsumption_Zone1']\n",
        "X = df[features]\n",
        "y = df['PowerConsumption_Zone1']\n",
        "\n",
        "# Standardize features\n",
        "scaler = StandardScaler()\n",
        "X_scaled = scaler.fit_transform(X)\n",
        "\n",
        "# Standardize target variable\n",
        "y_scaler = StandardScaler()\n",
        "y_scaled = y_scaler.fit_transform(y.values.reshape(-1, 1)).flatten()\n",
        "\n",
        "# Split data\n",
        "X_train, X_test, y_train, y_test = train_test_split(X_scaled, y_scaled, test_size=0.2, random_state=42)\n",
        "\n",
        "# Random Forest Regressor with warm_start for epoch simulation\n",
        "n_epochs = 100  # Number of trees (epochs)\n",
        "model = RandomForestRegressor(max_depth=10, random_state=42, n_jobs=-1, warm_start=True)\n",
        "\n",
        "train_losses = []\n",
        "val_losses = []\n",
        "\n",
        "# Simulate epochs by incrementally increasing n_estimators\n",
        "for epoch in range(1, n_epochs + 1):\n",
        "    model.n_estimators = epoch  # Set number of trees for this \"epoch\"\n",
        "    model.fit(X_train, y_train)\n",
        "\n",
        "    # Compute training and validation loss\n",
        "    y_pred_train = model.predict(X_train)\n",
        "    y_pred_val = model.predict(X_test)\n",
        "    train_loss = mean_squared_error(y_train, y_pred_train)\n",
        "    val_loss = mean_squared_error(y_test, y_pred_val)\n",
        "    train_losses.append(train_loss)\n",
        "    val_losses.append(val_loss)\n",
        "\n",
        "    # Print progress every 10 epochs\n",
        "    if epoch % 10 == 0:\n",
        "        print(f'Epoch {epoch}/{n_epochs}, Train Loss: {train_loss:.4f}, Val Loss: {val_loss:.4f}')\n",
        "\n",
        "# Final predictions\n",
        "y_pred_scaled = model.predict(X_test)\n",
        "y_pred = y_scaler.inverse_transform(y_pred_scaled.reshape(-1, 1)).flatten()\n",
        "y_test_orig = y_scaler.inverse_transform(y_test.reshape(-1, 1)).flatten()\n",
        "\n",
        "# Testing\n",
        "mse = mean_squared_error(y_test_orig, y_pred)\n",
        "r2 = r2_score(y_test_orig, y_pred)\n",
        "print(f'Mean Squared Error: {mse:.2f}')\n",
        "print(f'R² Score: {r2:.2f}')\n",
        "\n",
        "# Plotting actual vs predicted\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.plot(y_test_orig[:2000], label='Actual Power Consumption', marker='o')\n",
        "plt.plot(y_pred[:2000], label='Predicted Power Consumption', marker='x')\n",
        "plt.title('Actual vs Predicted Power Consumption (Zone 1) - Random Forest')\n",
        "plt.xlabel('Sample Index')\n",
        "plt.ylabel('Power Consumption')\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "plt.show()\n",
        "\n",
        "# Plotting loss over epochs\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.plot(range(1, n_epochs + 1), train_losses, label='Training Loss')\n",
        "plt.plot(range(1, n_epochs + 1), val_losses, label='Validation Loss')\n",
        "plt.title('Training and Validation Loss vs Epochs')\n",
        "plt.xlabel('Epoch (Number of Trees)')\n",
        "plt.ylabel('Mean Squared Error Loss (Scaled)')\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "plt.show()\n",
        "print(\"Data Summary:\")\n",
        "print(df.describe())\n",
        "print(\"\\nMissing Values:\")\n",
        "print(df.isnull().sum())\n",
        "# Note: If there are missing values, consider handling them, e.g., df.fillna(df.mean(), inplace=True)\n",
        "\n",
        "# Features (excluding the target variable to prevent data leakage)\n",
        "features = ['Temperature', 'Humidity', 'WindSpeed', 'GeneralDiffuseFlows', 'DiffuseFlows', 'Hour', 'Day', 'Month','PowerConsumption_Zone1']\n",
        "X = df[features]\n",
        "y = df['PowerConsumption_Zone1']\n",
        "\n",
        "# Standardize features\n",
        "scaler = StandardScaler()\n",
        "X_scaled = scaler.fit_transform(X)\n",
        "\n",
        "# Standardize target variable\n",
        "y_scaler = StandardScaler()\n",
        "y_scaled = y_scaler.fit_transform(y.values.reshape(-1, 1)).flatten()\n",
        "\n",
        "# Split data\n",
        "X_train, X_test, y_train, y_test = train_test_split(X_scaled, y_scaled, test_size=0.2, random_state=42)\n",
        "\n",
        "# XGBoost Regressor with epoch simulation\n",
        "n_epochs = 100  # Number of epochs (trees)\n",
        "train_losses = []\n",
        "val_losses = []\n",
        "\n",
        "# Simulate epochs by increasing n_estimators\n",
        "for epoch in range(1, n_epochs + 1):\n",
        "    model = XGBRegressor(n_estimators=epoch, learning_rate=0.1, max_depth=6, random_state=42, n_jobs=-1)\n",
        "    model.fit(X_train, y_train)\n",
        "\n",
        "    # Compute predictions\n",
        "    y_pred_train = model.predict(X_train)\n",
        "    y_pred_val = model.predict(X_test)\n",
        "\n",
        "    # Compute training and validation loss\n",
        "    train_loss = mean_squared_error(y_train, y_pred_train)\n",
        "    val_loss = mean_squared_error(y_test, y_pred_val)\n",
        "    train_losses.append(train_loss)\n",
        "    val_losses.append(val_loss)\n",
        "\n",
        "    # Print progress every 10 epochs\n",
        "    if epoch % 10 == 0:\n",
        "        print(f'Epoch {epoch}/{n_epochs}, Train Loss: {train_loss:.4f}, Val Loss: {val_loss:.4f}')\n",
        "\n",
        "# Final model training with full n_estimators\n",
        "model = XGBRegressor(n_estimators=n_epochs, learning_rate=0.1, max_depth=6, random_state=42, n_jobs=-1)\n",
        "model.fit(X_train, y_train)\n",
        "y_pred_scaled = model.predict(X_test)\n",
        "\n",
        "# Inverse transform for evaluation\n",
        "y_pred = y_scaler.inverse_transform(y_pred_scaled.reshape(-1, 1)).flatten()\n",
        "y_test_orig = y_scaler.inverse_transform(y_test.reshape(-1, 1)).flatten()\n",
        "\n",
        "# Testing\n",
        "mse = mean_squared_error(y_test_orig, y_pred)\n",
        "r2 = r2_score(y_test_orig, y_pred)\n",
        "print(f'Mean Squared Error: {mse:.2f}')\n",
        "print(f'R² Score: {r2:.2f}')\n",
        "\n",
        "# Plotting actual vs predicted (plot a range of values)\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.plot(y_test_orig[:2000], label='Actual Power Consumption', marker='o', linestyle='-', color='blue')\n",
        "plt.plot(y_pred[:2000], label='Predicted Power Consumption', marker='x', linestyle='--', color='orange')\n",
        "plt.title('Actual vs Predicted Power Consumption (Zone 1) - XGBoost')\n",
        "plt.xlabel('Sample Index')\n",
        "plt.ylabel('Power Consumption')\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "plt.show()\n",
        "\n",
        "# Plotting loss over epochs\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.plot(range(1, n_epochs + 1), train_losses, label='Training Loss', color='blue')\n",
        "plt.plot(range(1, n_epochs + 1), val_losses, label='Validation Loss', color='orange')\n",
        "plt.title('Training and Validation Loss vs Epochs')\n",
        "plt.xlabel('Epoch (Number of Trees)')\n",
        "plt.ylabel('Mean Squared Error Loss (Scaled)')\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "plt.show()\n",
        "\n",
        "# Feature importance plot\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.bar(features, model.feature_importances_)\n",
        "plt.title('Feature Importance - XGBoost')\n",
        "plt.xlabel('Features')\n",
        "plt.ylabel('Importance')\n",
        "plt.xticks(rotation=45)\n",
        "plt.grid(True)\n",
        "plt.show()\n",
        "print(\"Data Summary:\")\n",
        "print(df.describe())\n",
        "print(\"\\nMissing Values:\")\n",
        "print(df.isnull().sum())\n",
        "\n",
        "# Features (consistent with previous code)\n",
        "features = ['Temperature', 'Humidity', 'WindSpeed', 'GeneralDiffuseFlows', 'DiffuseFlows', 'Hour', 'Day', 'Month','PowerConsumption_Zone1']\n",
        "X = df[features]\n",
        "y = df['PowerConsumption_Zone1']\n",
        "\n",
        "# Standardize features\n",
        "scaler = StandardScaler()\n",
        "X_scaled = scaler.fit_transform(X)\n",
        "\n",
        "# Standardize target variable\n",
        "y_scaler = StandardScaler()\n",
        "y_scaled = y_scaler.fit_transform(y.values.reshape(-1, 1)).flatten()\n",
        "\n",
        "# Split data\n",
        "X_train, X_test, y_train, y_test = train_test_split(X_scaled, y_scaled, test_size=0.2, random_state=42)\n",
        "\n",
        "# Simulate epochs by incrementally increasing training data\n",
        "n_epochs = 100\n",
        "train_losses = []\n",
        "val_losses = []\n",
        "n_samples = len(X_train)\n",
        "step_size = max(1, n_samples // n_epochs)  # Ensure at least 1 sample per step\n",
        "\n",
        "# Initialize model\n",
        "model = SVR(kernel='rbf', C=100, epsilon=0.1)\n",
        "\n",
        "for epoch in range(n_epochs):\n",
        "    # Use increasing subsets of training data\n",
        "    end_idx = min((epoch + 1) * step_size, n_samples)\n",
        "    X_train_subset = X_train[:end_idx]\n",
        "    y_train_subset = y_train[:end_idx]\n",
        "\n",
        "    # Train model on subset\n",
        "    model.fit(X_train_subset, y_train_subset)\n",
        "\n",
        "    # Compute training and validation loss\n",
        "    y_pred_train = model.predict(X_train_subset)\n",
        "    y_pred_val = model.predict(X_test)\n",
        "    train_loss = mean_squared_error(y_train_subset, y_pred_train)\n",
        "    val_loss = mean_squared_error(y_test, y_pred_val)\n",
        "    train_losses.append(train_loss)\n",
        "    val_losses.append(val_loss)\n",
        "\n",
        "    # Print progress every 10 epochs\n",
        "    if (epoch + 1) % 10 == 0:\n",
        "        print(f'Epoch {epoch + 1}/{n_epochs}, Train Loss: {train_loss:.4f}, Val Loss: {val_loss:.4f}')\n",
        "\n",
        "# Final model training on full training data\n",
        "model.fit(X_train, y_train)\n",
        "y_pred_scaled = model.predict(X_test)\n",
        "\n",
        "# Inverse transform for evaluation\n",
        "y_pred = y_scaler.inverse_transform(y_pred_scaled.reshape(-1, 1)).flatten()\n",
        "y_test_orig = y_scaler.inverse_transform(y_test.reshape(-1, 1)).flatten()\n",
        "\n",
        "# Testing\n",
        "mse = mean_squared_error(y_test_orig, y_pred)\n",
        "r2 = r2_score(y_test_orig, y_pred)\n",
        "print(f'Mean Squared Error: {mse:.2f}')\n",
        "print(f'R² Score: {r2:.2f}')\n",
        "\n",
        "# Plotting actual vs predicted\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.plot(y_test_orig[:2000], label='Actual Power Consumption', marker='o')\n",
        "plt.plot(y_pred[:2000], label='Predicted Power Consumption', marker='x')\n",
        "plt.title('Actual vs Predicted Power Consumption (Zone 1) - SVR')\n",
        "plt.xlabel('Sample Index')\n",
        "plt.ylabel('Power Consumption')\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "plt.show()\n",
        "\n",
        "# Plotting loss over epochs\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.plot(range(1, n_epochs + 1), train_losses, label='Training Loss')\n",
        "plt.plot(range(1, n_epochs + 1), val_losses, label='Validation Loss')\n",
        "plt.title('Training and Validation Loss vs Epochs')\n",
        "plt.xlabel('Epoch (Data Subset Iteration)')\n",
        "plt.ylabel('Mean Squared Error Loss (Scaled)')\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import mean_squared_error, r2_score\n",
        "from sklearn.tree import DecisionTreeRegressor\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "from xgboost import XGBRegressor\n",
        "from sklearn.svm import SVR\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import LSTM, Dense\n",
        "from tensorflow.keras.callbacks import EarlyStopping\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "# Load and preprocess data\n",
        "df = pd.read_csv('loadpredictiondataset.csv')\n",
        "df['Datetime'] = pd.to_datetime(df['Datetime'], format='%m/%d/%Y %H:%M')\n",
        "df['Hour'] = df['Datetime'].dt.hour\n",
        "df['Day'] = df['Datetime'].dt.day\n",
        "df['Month'] = df['Datetime'].dt.month\n",
        "df = df.dropna()\n",
        "\n",
        "# Outlier removal using IQR\n",
        "def remove_outliers(df, column):\n",
        "    Q1 = df[column].quantile(0.25)\n",
        "    Q3 = df[column].quantile(0.75)\n",
        "    IQR = Q3 - Q1\n",
        "    lower_bound = Q1 - 1.5 * IQR\n",
        "    upper_bound = Q3 + 1.5 * IQR\n",
        "    return df[(df[column] >= lower_bound) & (df[column] <= upper_bound)]\n",
        "\n",
        "df_clean = remove_outliers(df, 'PowerConsumption_Zone1')\n",
        "\n",
        "# Prepare features and target (excluding PowerConsumption_Zone1 to avoid leakage)\n",
        "features = ['Temperature', 'Humidity', 'WindSpeed', 'GeneralDiffuseFlows', 'DiffuseFlows', 'Hour', 'Day', 'Month']\n",
        "X = df_clean[features]\n",
        "y = df_clean['PowerConsumption_Zone1']\n",
        "\n",
        "# Standardize features and target\n",
        "scaler = StandardScaler()\n",
        "X_scaled = scaler.fit_transform(X)\n",
        "y_scaler = StandardScaler()\n",
        "y_scaled = y_scaler.fit_transform(y.values.reshape(-1, 1)).flatten()\n",
        "\n",
        "# Split data\n",
        "X_train, X_test, y_train, y_test = train_test_split(X_scaled, y_scaled, test_size=0.2, random_state=42)\n",
        "\n",
        "# Custom Linear Regression with L2 Regularization\n",
        "class CustomLinearRegression:\n",
        "    def __init__(self, learning_rate=0.0008, epochs=1500, l2_lambda=0.05):\n",
        "        self.learning_rate = learning_rate\n",
        "        self.epochs = epochs\n",
        "        self.l2_lambda = l2_lambda\n",
        "        self.weights = None\n",
        "        self.bias = None\n",
        "\n",
        "    def fit(self, X, y):\n",
        "        n_samples, n_features = X.shape\n",
        "        self.weights = np.zeros(n_features)\n",
        "        self.bias = 0\n",
        "        for _ in range(self.epochs):\n",
        "            y_pred = np.dot(X, self.weights) + self.bias\n",
        "            dw = (1/n_samples) * np.dot(X.T, (y_pred - y)) + 2 * self.l2_lambda * self.weights\n",
        "            db = (1/n_samples) * np.sum(y_pred - y)\n",
        "            self.weights -= self.learning_rate * dw\n",
        "            self.bias -= self.learning_rate * db\n",
        "\n",
        "    def predict(self, X):\n",
        "        return np.dot(X, self.weights) + self.bias\n",
        "\n",
        "# Train models and collect predictions\n",
        "models = {\n",
        "    'Custom Linear Regression': CustomLinearRegression(learning_rate=0.0008, epochs=1500, l2_lambda=0.05),\n",
        "    'Decision Tree': DecisionTreeRegressor(max_depth=10, random_state=42),\n",
        "    'Random Forest': RandomForestRegressor(max_depth=10, random_state=42, n_jobs=-1),\n",
        "    'XGBoost': XGBRegressor(n_estimators=100, learning_rate=0.1, max_depth=6, random_state=42, n_jobs=-1),\n",
        "    'SVR': SVR(kernel='rbf', C=100, epsilon=0.1)\n",
        "}\n",
        "\n",
        "# Store predictions\n",
        "predictions = {}\n",
        "y_test_orig = y_scaler.inverse_transform(y_test.reshape(-1, 1)).flatten()\n",
        "\n",
        "for name, model in models.items():\n",
        "    model.fit(X_train, y_train)\n",
        "    y_pred_scaled = model.predict(X_test)\n",
        "    predictions[name] = y_scaler.inverse_transform(y_pred_scaled.reshape(-1, 1)).flatten()\n",
        "    mse = mean_squared_error(y_test_orig, predictions[name])\n",
        "    r2 = r2_score(y_test_orig, predictions[name])\n",
        "    print(f'{name} - MSE: {mse:.2f}, R²: {r2:.2f}')\n",
        "\n",
        "# LSTM Model\n",
        "timesteps = 10\n",
        "X_lstm = np.array([X_scaled[i:i+timesteps] for i in range(len(X_scaled) - timesteps)])\n",
        "y_lstm = y_scaled[timesteps:]\n",
        "X_train_lstm, X_test_lstm, y_train_lstm, y_test_lstm = train_test_split(X_lstm, y_lstm, test_size=0.2, random_state=42)\n",
        "\n",
        "model_lstm = Sequential([\n",
        "    LSTM(50, activation='relu', input_shape=(timesteps, X_scaled.shape[1]), return_sequences=False),\n",
        "    Dense(1)\n",
        "])\n",
        "model_lstm.compile(optimizer='adam', loss='mse')\n",
        "early_stopping = EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)\n",
        "model_lstm.fit(X_train_lstm, y_train_lstm, epochs=100, batch_size=32, validation_data=(X_test_lstm, y_test_lstm),\n",
        "               callbacks=[early_stopping], verbose=0)\n",
        "\n",
        "y_pred_scaled_lstm = model_lstm.predict(X_test_lstm, verbose=0)\n",
        "predictions['LSTM'] = y_scaler.inverse_transform(y_pred_scaled_lstm).flatten()\n",
        "mse_lstm = mean_squared_error(y_test_orig[:len(predictions['LSTM'])], predictions['LSTM'])\n",
        "r2_lstm = r2_score(y_test_orig[:len(predictions['LSTM'])], predictions['LSTM'])\n",
        "print(f'LSTM - MSE: {mse_lstm:.2f}, R²: {r2_lstm:.2f}')\n",
        "\n",
        "# Combined Actual vs Predicted Plot\n",
        "plt.figure(figsize=(12, 8))\n",
        "plt.plot(y_test_orig[:100], label='Actual', color='black', marker='o', linestyle='-')\n",
        "colors = ['red', 'blue', 'green', 'orange', 'purple', 'cyan']\n",
        "for idx, (name, y_pred) in enumerate(predictions.items()):\n",
        "    plt.plot(y_pred[:100], label=name, color=colors[idx], marker='x', linestyle='--')\n",
        "plt.title('Actual vs Predicted Power Consumption (Zone 1)')\n",
        "plt.xlabel('Sample Index')\n",
        "plt.ylabel('Power Consumption')\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "plt.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IiVhG7h1VjV0",
        "outputId": "f3b0cf78-2699-4427-96fb-d5c36dc469e1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Custom Linear Regression - MSE: 21379541.25, R²: 0.58\n",
            "Decision Tree - MSE: 3282796.56, R²: 0.93\n",
            "Random Forest - MSE: 2693060.31, R²: 0.95\n",
            "XGBoost - MSE: 2230540.82, R²: 0.96\n"
          ]
        }
      ]
    }
  ]
}